{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import uuid\n",
    "from numpy import sort\n",
    "\n",
    "import peh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_path = Path(\"../source_tables/PARC/BasicCodebook_v2.3.xlsx\")\n",
    "data_path = Path(\"../source_tables/PARC/ExData_BasicCodebook_v2.3.xlsx\")\n",
    "yaml_file_path = Path(\"../project_examples/PARC/parc.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pd.read_excel(data_path, sheet_name=None)\n",
    "for k, v in data_dict.items():\n",
    "    data_dict[k] = v.replace(np.nan, None)\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities\n",
    "The entities we will use for this data are:\n",
    "- EntityList\n",
    "    - StudyEntity\n",
    "        - Study (example-study) \n",
    "        - Person\n",
    "        - Sample\n",
    "        - PersonGroup\n",
    "        - SampleCollection (equivalent to one of the tabs)\n",
    "        - Timepoint\n",
    "\n",
    "Additionally, we want to extract properties for these entities from the BasicCodebook, but that's for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [x for x in dir(peh) if \"sampl\" in x.lower()]\n",
    "print(s)\n",
    "print(dir(peh.SamplingResult))\n",
    "print(dir(peh.SamplingObservation))\n",
    "print(peh.SamplingResult())\n",
    "print(peh.SamplingObservation(id=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entitylist = peh.EntityList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sheet = data_dict[\"STUDYINFO\"]\n",
    "study = peh.Study(id=study_sheet.iloc[11][1])\n",
    "entitylist.studies = [study]\n",
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints_sheet = data_dict[\"TIMEPOINT\"]\n",
    "\n",
    "timepoints = {}\n",
    "for i, row in timepoints_sheet.iterrows():\n",
    "    tp = peh.Timepoint(id=peh.TimepointId(row[\"id_timepoint\"]))\n",
    "    timepoints[row[\"id_timepoint\"]] = tp\n",
    "entitylist.timepoints = timepoints\n",
    "study.timepoint_id_list = [peh.TimepointId(x) for x in timepoints.keys()]\n",
    "timepoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE\n",
    "sample_sheet = data_dict[\"SAMPLE\"]\n",
    "\n",
    "# where to save these?\n",
    "samples = []\n",
    "\n",
    "# SamplingObservation > SamplingResult > ObservedValue\n",
    "sampling_design = peh.SamplingDesign()\n",
    "to_ignore = {\"id_sample\", \"id_subject\", \"id_timepoint\"}\n",
    "special_fields = {\"chol\", \"trigl\", \"lipid\", \"lipid_enz\", \"crt\", \"sg\", \"osm\", \"density\", \"lipid_enz_harm\"}\n",
    "for _, row in sample_sheet.iterrows():\n",
    "    sample = peh.Sample(id=peh.SampleId(row[\"id_sample\"]))\n",
    "    samples.append(sample)\n",
    "    meta_values = []\n",
    "    sample_values = []\n",
    "    obs = []\n",
    "    for idx, val in row.items():\n",
    "        if idx not in to_ignore:\n",
    "            if idx not in special_fields:\n",
    "                meta_values.append(peh.ObservedValue(observable_entity=sample.id, value=val, observable_property=idx))\n",
    "            else:\n",
    "                sample_values.append(peh.ObservedValue(observable_entity=sample.id, value=val, observable_property=idx))\n",
    "\n",
    "    meta_res = peh.SamplingResult(observed_values=meta_values)\n",
    "    sampling_res = peh.SamplingResult(observed_values=sample_values)\n",
    "    obs.append(peh.SamplingObservation(id=peh.SamplingObservationId(uuid.uuid4()), observation_result=sampling_res, observation_design=sampling_design, observation_type=peh.ObservationType.sampling))\n",
    "    obs.append(peh.SamplingObservation(id=peh.SamplingObservationId(uuid.uuid4()), observation_result=meta_res, observation_design=sampling_design, observation_type=peh.ObservationType.metadata))\n",
    "    timepoints[row[\"id_timepoint\"]].observations.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_unique_sheet = data_dict[\"SUBJECTUNIQUE\"]\n",
    "subject_design = peh.QuestionnaireDesign()\n",
    "to_ignore = {\"id_subject\", \"id_participant\"}\n",
    "\n",
    "# where to save these\n",
    "subjects = []\n",
    "\n",
    "# link persons to each other\n",
    "subject_groups = {i: peh.PersonGroup(i) for i in set(subject_unique_sheet[\"id_participant\"].values)}\n",
    "\n",
    "for _, row in subject_unique_sheet.iterrows():\n",
    "    person = peh.Person(id=peh.PersonId(row[\"id_subject\"]))\n",
    "    subjects.append(person)\n",
    "    \n",
    "    values = []\n",
    "    obs = []\n",
    "    for idx, val in row.items():\n",
    "        if idx not in to_ignore:\n",
    "            values.append(peh.ObservedValue(observable_entity=person.id, value=val, observable_property=idx))\n",
    "    subject_groups[row[\"id_participant\"]].study_entity_links.append(peh.StudyEntityLink(study_entity=peh.StudyEntityId(row[\"id_participant\"]), linktype=peh.LinkType.is_part_of))\n",
    "    res = peh.QuestionnaireResult(observed_values=values)\n",
    "    obs.append(peh.QuestionnaireObservation(id=peh.QuestionnaireObservationId(uuid.uuid4()), observation_result=res, observation_design=subject_design, observation_type=peh.ObservationType.questionnaire))\n",
    "    # no timepoints linked, so we use the first one. Assumes these are somehow alphabetically or numerically sortable\n",
    "    timepoints[sort(list(timepoints.keys()))[0]].observations.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_timepoint_sheet = data_dict[\"SUBJECTTIMEPOINT\"]\n",
    "to_ignore = {\"id_subject\", \"id_timepoint\"}\n",
    "\n",
    "for _, row in subject_timepoint_sheet.iterrows():\n",
    "    values = []\n",
    "    obs = []\n",
    "    for idx, val in row.items():\n",
    "        if idx not in to_ignore:\n",
    "            values.append(peh.ObservedValue(observable_entity=person.id, value=val, observable_property=idx))\n",
    "    res = peh.QuestionnaireResult(observed_values=values)\n",
    "    obs.append(peh.QuestionnaireObservation(id=peh.QuestionnaireObservationId(uuid.uuid4()), observation_result=res, observation_design=subject_design, observation_type=peh.ObservationType.questionnaire))\n",
    "    # no timepoints linked, so we use the first one. Assumes these are somehow alphabetically or numerically sortable\n",
    "    timepoints[row[\"id_timepoint\"]].observations.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletimepoint_sheets = {k: v for k, v in data_dict.items() if str(k).startswith(\"SAMPLETIMEPOINT\")}\n",
    "\n",
    "timepoints_lookup = sample_sheet[[\"id_sample\", \"id_timepoint\"]]\n",
    "\n",
    "for k, v in sampletimepoint_sheets.items():\n",
    "    # SamplingObservation > SamplingResult > ObservedValue\n",
    "    sheet = v.merge(timepoints_lookup, on=\"id_sample\")\n",
    "    sampling_design = peh.SamplingDesign()\n",
    "    orig = {i[:-4] for i in sheet.columns if i.endswith(\"_lod\") or i.endswith(\"_loq\")}\n",
    "    markers = orig - special_fields\n",
    "    to_ignore = {\"id_sample\"}\n",
    "    for _, row in sheet.iterrows():\n",
    "        values = []\n",
    "        obs = []\n",
    "        for m in markers:\n",
    "            values.append(peh.ObservedValue(observable_entity=peh.SampleId(row[\"id_sample\"]), value=row[str(m)], observable_property=str(m),\n",
    "                                            quality_data=[peh.QualityData(quality_context_key=\"lod\", quality_value=row[str(m) + \"_lod\"]),\n",
    "                                                          peh.QualityData(quality_context_key=\"loq\", quality_value=row[str(m) + \"_loq\"])]\n",
    "                                            ))\n",
    "\n",
    "        sampling_res = peh.SamplingResult(observed_values=values)\n",
    "        obs.append(peh.SamplingObservation(id=peh.SamplingObservationId(uuid.uuid4()), observation_result=sampling_res, observation_design=sampling_design, observation_type=peh.ObservationType.sampling))\n",
    "        \n",
    "        timepoints[row[\"id_timepoint\"]].observations.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml_runtime.dumpers import yaml_dumper\n",
    "\n",
    "\n",
    "yaml_dumper.dump(entitylist, Path(\"out/PARC/data.yaml\"))\n",
    "# list all samples\n",
    "yaml_dumper.dump(samples, Path(\"out/PARC/samples.yaml\"))\n",
    "# list all subjects\n",
    "yaml_dumper.dump(subjects, Path(\"out/PARC/persons.yaml\"))\n",
    "yaml_dumper.dump(list(subject_groups.values()), Path(\"out/PARC/person_groups.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml_runtime.dumpers import json_dumper\n",
    "cwd = Path.cwd()\n",
    "\n",
    "json_dumper.dump(entitylist, cwd / \"out/PARC/data.jsonld\", contexts=str(cwd / \"out/peh.jsonld\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Codebook/Reference data for validation\n",
    "\n",
    "Needed information for validation:\n",
    "- project: str\n",
    "- sheet_name: str\n",
    "- name: str\n",
    "- control_type: str\n",
    "- control_value: str\n",
    "- level: str\n",
    "- cond_type: str = np.nan\n",
    "- cond_param_name: str = np.nan\n",
    "- cond_value: str = np.nan\n",
    "- message: str = np.nan\n",
    "- strictness: str = \"any\"\n",
    "- p50/95 per biomarker\n",
    "- allowed biomarkers per study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BIOCHEMENTITIES = \"BioChemEntityList_data.yaml\"\n",
    "BIOCHEMGROUPING = \"BioChemGroupingList_data.yaml\"\n",
    "INDICATORLIST = \"IndicatorList_data.yaml\"\n",
    "MATRIXLIST = \"MatrixList_data.yaml\"\n",
    "OBSERVABLEPROPERTYGROUPLIST = \"ObservablePropertyGroupList_data.yaml\"\n",
    "OBSERVABLEPROPERTYLIST = \"ObservablePropertyList_data.yaml\"\n",
    "OBSERVABLEPROPERTYMETADATALIST = \"ObservablePropertyMetadataFieldList_data.yaml\"\n",
    "PROJECTLIST = \"ProjectList_data.yaml\"\n",
    "STAKEHOLDERLIST = \"StakeholderList_data.yaml\"\n",
    "STUDYLIST = \"StudyList_data.yaml\"\n",
    "TIMEPOINTLIST = \"TimepointList_data.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "from linkml_runtime.loaders import yaml_loader\n",
    "\n",
    "\n",
    "def create_entity_list(base_path: Path) -> peh.EntityList:\n",
    "    el = peh.EntityList(\n",
    "        matrices=yaml_loader.load(str(base_path / MATRIXLIST), peh.EntityList).matrices,\n",
    "        metadata_fields = yaml_loader.load(str(base_path / OBSERVABLEPROPERTYMETADATALIST), peh.EntityList).metadata_fields,\n",
    "        biochementities=yaml_loader.load(str(base_path / BIOCHEMENTITIES), peh.EntityList).biochementities,\n",
    "        biochemgroupings=yaml_loader.load(str(base_path / BIOCHEMGROUPING), peh.EntityList).biochemgroupings,\n",
    "        indicators=yaml_loader.load(str(base_path / INDICATORLIST), peh.EntityList).indicators,\n",
    "        # units= ...,\n",
    "        observable_property_groups=yaml_loader.load(str(base_path / OBSERVABLEPROPERTYGROUPLIST), peh.EntityList).observable_property_groups,\n",
    "        observable_properties=yaml_loader.load(str(base_path / OBSERVABLEPROPERTYLIST), peh.EntityList).observable_properties,\n",
    "        stakeholders=yaml_loader.load(str(base_path / STAKEHOLDERLIST), peh.EntityList).stakeholders,\n",
    "        projects=yaml_loader.load(str(base_path / PROJECTLIST), peh.EntityList).projects,\n",
    "        studies=yaml_loader.load(str(base_path / STUDYLIST), peh.EntityList).studies,\n",
    "        timepoints=yaml_loader.load(str(base_path / TIMEPOINTLIST), peh.EntityList).timepoints,\n",
    "    )\n",
    "\n",
    "    return el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PurePath(\"/mnt/c/Users/PEETERSR/projects/parco-hbm/linkml/data\")\n",
    "el = create_entity_list(path)\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_dict = vars(el)\n",
    "el_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the EntityList as input for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_obs_props(el: peh.EntityList, id:str):\n",
    "    return next((obs for obs in el.observable_properties if obs.id == id), None)\n",
    "\n",
    "def get_raw_value_metadata(el: peh.EntityList, id: str, field: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found:\n",
    "        return None\n",
    "    for md in found.value_metadata:\n",
    "        if md.field == field:\n",
    "            return md.value\n",
    "\n",
    "def get_type(el: peh.EntityList, id: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found:\n",
    "        return None\n",
    "    return \"categorical\" if found.categorical else found.value_type\n",
    "\n",
    "def get_allowed_values(el: peh.EntityList, id: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found or not found.categorical:\n",
    "        return None\n",
    "    return {v.key : v.value for v in found.value_options}\n",
    "    \n",
    "def get_conditional(el: peh.EntityList, id: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found or not found.validation_design:\n",
    "        return None\n",
    "    return found.validation_design.conditional\n",
    "\n",
    "def get_def_significant_decimals(el: peh.EntityList, id: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found:\n",
    "        return None\n",
    "    return found.default_significantdecimals\n",
    "\n",
    "def get_def_unit(el: peh.EntityList, id: str):\n",
    "    found = find_first_obs_props(el, id)\n",
    "    if not found:\n",
    "        return None\n",
    "    return found.default_unit\n",
    "\n",
    "# example\n",
    "print(get_raw_value_metadata(el, \"height\", \"min\"))\n",
    "print(get_type(el, \"height\"))\n",
    "print(get_allowed_values(el, \"isced_hh\"))\n",
    "print(get_conditional(el, \"isced_hh\"))\n",
    "print(get_def_significant_decimals(el, \"height\"))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get validation configuration from reference data\n",
    "\n",
    "(auto)\n",
    "- min\n",
    "- max\n",
    "- allowed values\n",
    "- decimals after comma\n",
    "- type\n",
    "- required\n",
    "- conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QcReq:\n",
    "    min: float\n",
    "    max: float\n",
    "    type: str\n",
    "    allowed_values: dict\n",
    "    conditional: str\n",
    "    precision: float\n",
    "    required: bool\n",
    "    unit: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "INDICATOR = \"height\"\n",
    "\n",
    "#  Required is still missing\n",
    "\n",
    "reqs = QcReq(\n",
    "    min = get_raw_value_metadata(el, INDICATOR, \"min\"),\n",
    "    max = get_raw_value_metadata(el, INDICATOR, \"max\"),\n",
    "    type = get_type(el, INDICATOR),\n",
    "    allowed_values = get_allowed_values(el, INDICATOR),\n",
    "    conditional = get_conditional(el, INDICATOR),\n",
    "    precision = get_def_significant_decimals(el, INDICATOR),\n",
    "    required = True,\n",
    "    # currently not needed for validation\n",
    "    unit = get_def_unit(el, INDICATOR),\n",
    ")\n",
    "\n",
    "print(f\"{INDICATOR=}\")\n",
    "pprint(reqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
